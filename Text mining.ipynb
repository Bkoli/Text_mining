{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['In', 'Brazil', 'they', 'drive', 'on', 'the', 'right-hand', 'side', 'of', 'the', 'road', '.', 'Brazil', 'has', 'a', 'large', 'coastline', 'on', 'the', 'eastern', 'side', 'of', 'South', 'America']\n"
     ]
    }
   ],
   "source": [
    "# Text Analysis Operations using NLTK\n",
    "# Tokenization\n",
    "# Stopwords\n",
    "# Lexicon Normalization such as Stemming and Lemmatization\n",
    "# POS Tagging\n",
    "\n",
    "\n",
    "# Importing necessary library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "import nltk.corpus\n",
    "# sample text for performing tokenization\n",
    "text = \"In Brazil they drive on the right-hand side of the road. Brazil has a large coastline on the eastern side of South America\"\n",
    "# importing word_tokenize from nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Tokenization\n",
    "# Passing the string text into word tokenize for breaking the sentences\n",
    "token = word_tokenize(text)\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freuency distinct in the text {'In': 1, 'Brazil': 2, 'they': 1, 'drive': 1, 'on': 2, 'the': 3, 'right-hand': 1, 'side': 2, 'of': 2, 'road': 1, '.': 1, 'has': 1, 'a': 1, 'large': 1, 'coastline': 1, 'eastern': 1, 'South': 1, 'America': 1}\n"
     ]
    }
   ],
   "source": [
    "# Finding frequency distinct in the text\n",
    "\n",
    "# finding the frequency distinct in the tokens\n",
    "# Importing FreqDist library from nltk and passing token into FreqDist\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(token) \n",
    "print(\"Freuency distinct in the text\",dict(fdist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common words: {'the': 3, 'Brazil': 2, 'on': 2, 'side': 2, 'of': 2, 'In': 1, 'they': 1, 'drive': 1, 'right-hand': 1, 'road': 1}\n"
     ]
    }
   ],
   "source": [
    "# ‘the’ is found 3 times in the text, ‘Brazil’ is found 2 times in the text, etc.\n",
    "\n",
    "# To find the frequency of top 10 words\n",
    "fdist1 = fdist.most_common(10)\n",
    "print(\"frequency of top 10 words:\",dict(fdist1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wait'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming\n",
    "# Stemming usually refers to normalizing words into its base form or root form.\n",
    "# 1.Porter Stemming (removes common morphological and inflectional endings from words)\n",
    "# 2.Lancaster Stemming (a more aggressive stemming algorithm).\n",
    "\n",
    "\n",
    "# Importing Porterstemmer from nltk library\n",
    "# Checking for the word ‘giving’ \n",
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()\n",
    "pst.stem(\"waiting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waited:wait\n",
      "waiting:wait\n",
      "waits:wait\n"
     ]
    }
   ],
   "source": [
    "# Checking for the list of words\n",
    "stm = [\"waited\", \"waiting\", \"waits\"]\n",
    "for word in stm :\n",
    "   print(word+ \":\" +pst.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giving:giv\n",
      "given:giv\n",
      "given:giv\n",
      "gave:gav\n"
     ]
    }
   ],
   "source": [
    "# Lancaster is more aggressive than Porter stemmer\n",
    "# Importing LancasterStemmer from nltk\n",
    "from nltk.stem import LancasterStemmer\n",
    "lst = LancasterStemmer()\n",
    "stm = [\"giving\", \"given\", \"given\", \"gave\"]\n",
    "for word in stm :\n",
    " print(word+ \":\" + lst.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "waited:wait\n",
      "waiting:wait\n",
      "waits:wait\n"
     ]
    }
   ],
   "source": [
    "# Checking for the list of words\n",
    "stm = [\"waited\", \"waiting\", \"waits\"]\n",
    "for word in stm :\n",
    "   print(word+ \":\" +lst.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "copora : corpus\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "# In simpler terms, it is the process of converting a word to its base form. \n",
    "# The difference between stemming and lemmatization is, lemmatization considers the context \n",
    "# and converts the word to its meaningful base form, whereas stemming just removes the last few characters,\n",
    "# often leading to incorrect meanings and spelling errors.\n",
    "\n",
    "# importing Lemmatizer library from nltk\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"copora :\", lemmatizer.lemmatize(\"corpora\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert all letter to lower case:  ['christiano', 'ronaldo', 'was', 'born', 'on', 'febraury', '5', ',', '1986', ',', 'in', 'funchal', ',', 'madeira', ',', 'portugal']\n",
      "tokens after stopwords removal:  ['christiano', 'ronaldo', 'born', 'febraury', '5', ',', '1986', ',', 'funchal', ',', 'madeira', ',', 'portugal']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bhim.DESKTOP-3662N79\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Stop Words\n",
    "# “Stop words” are the most common words in a language like “the”, “a”, “at”, “for”, “above”, “on”, “is”, “all”. \n",
    "# These words do not provide any meaning and are usually removed from texts.\n",
    "# We can remove these stop words using nltk library\n",
    "#nltk.download('stopwords') nltk needs to download special requirement using download method for once.\n",
    "\n",
    "# import stopwords from nltk.corpus\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "a= set(stopwords.words('english'))\n",
    "\n",
    "text = \"Christiano Ronaldo was born on Febraury 5, 1986, in Funchal, Madeira, Portugal\"\n",
    "\n",
    "text1 = word_tokenize(text.lower())\n",
    "print(\"Convert all letter to lower case: \", text1)\n",
    "\n",
    "stopwords= [x for x in text1 if x not in a]\n",
    "print(\"/n tokens after stopwords removal: \"  , stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\bhim.DESKTOP-3662N79\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('vote', 'NN')]\n",
      "[('to', 'TO')]\n",
      "[('choose', 'NN')]\n",
      "[('a', 'DT')]\n",
      "[('particular', 'JJ')]\n",
      "[('man', 'NN')]\n",
      "[('or', 'CC')]\n",
      "[('a', 'DT')]\n",
      "[('group', 'NN')]\n",
      "[('(', '(')]\n",
      "[('party', 'NN')]\n",
      "[(')', ')')]\n",
      "[('to', 'TO')]\n",
      "[('represent', 'NN')]\n",
      "[('them', 'PRP')]\n",
      "[('in', 'IN')]\n",
      "[('parliament', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# Part of speech tagging (POS)\n",
    "# Part-of-speech tagging is used to assign parts of speech to each word of a given text (\n",
    "#     such as nouns, verbs, pronouns, adverbs, conjunction, adjectives, interjection) \n",
    "# based on its definition and its context. \n",
    "# There are many tools available for POS taggers \n",
    "# and some of the widely used taggers are NLTK, Spacy, TextBlob, Standford CoreNLP, etc.\n",
    "\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "text = \"vote to choose a particular man or a group (party) to represent them in parliament\"\n",
    "#Tokenize the text\n",
    "tex = word_tokenize(text)\n",
    "for token in tex:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\bhim.DESKTOP-3662N79\\AppData\\Roaming\\nltk_dat\n",
      "[nltk_data]     a...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named entity recognition\n",
    "# It is the process of detecting the named entities\n",
    "# such as the person name,the location name, the company name, the quantities and the monetary value.\n",
    "\n",
    "text = \"Google's CEO Sunder Pichai introduced new Pixel at Minnesota Roi Centre Event\"\n",
    "\n",
    "#importing chunk library from nltk\n",
    "from nltk import ne_chunk\n",
    "# tokenize and POS Tagging before doing chunk\n",
    "token = word_tokenize(text)\n",
    "tags = nltk.pos_tag(token)\n",
    "chunk = ne_chunk(tags)\n",
    "chunk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
